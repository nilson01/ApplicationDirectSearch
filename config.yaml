# Network parameters configuration

# Models to run True False
run_surr_opt: True
run_DQlearning: True
run_adaptive_contrast_tao: True 
setting: 'scheme_8' # 'linear', 'tao', 'scheme_5', 'scheme_6', 'scheme_7', 'scheme_8' 

# scheme_5 constants 
C1: 3.0 
C2: 3.0 
beta: 1 
cnst: 1 # 1, 5, 10, 20, 30, 100 
 
# # scheme_6 constants 
# C1: 5.0 
# C2: 5.0 
# beta: 1 
# cnst: 10 #10
b: 2 # keep this 2 for scheme 8

# scheme 8
neu: 10
u: 10
alpha: 10

 

f_model: 'model_name'  # DQlearning, surr_opt, tao - igore here
noiseless: True # True, False  # Boolean flag to indicate if noise should be excluded in simulations
use_m_propen: True # True, False
interaction_terms: False

surrogate_num: 1  # Indicates the surrogate model configuration number
option_sur: 4  # Specifies the operational mode or variant of the surrogate model
device: None  # Computation device, dynamically set to 'cuda' if GPU is available
sample_size: 3000 # 30000  # Number of samples to be used 
batch_size: 300 #7000  # Batch size calculated as a proportion of sample size
num_replications: 5 # 30, 100
# job_id: tao 

training_validation_prop: 0.7  # Proportion of data for training vs validation
n_epoch: 250 # 150  # Number of training epochs
num_networks: 2  # Number of parallel networks or models
activation_function: 'elu' # elu, relu, sigmoid, tanh, leakyrelu, none

#  Input dimension for stage 1, [O1] --> [x1, x2, x3, x4, x5] input_dim_stage1: 5 
#  Input dimension for stage 2, includes [O1, A1, Y1, O2] input_dim_stage2: 7 

output_dim_stage1: 1  # Output dimension for stage 1
output_dim_stage2: 1  # Output dimension for stage 2
hidden_dim_stage1: 40  # Number of neurons in the hidden layer of stage 1
hidden_dim_stage2: 40  # Number of neurons in the hidden layer of stage 2 
dropout_rate: 0.4  # Dropout rate to prevent overfitting
num_layers: 0
optimizer_type: 'adam'  # Optimizer type, can be 'adam' or 'rmsprop'
optimizer_lr: 0.07  # Learning rate for the optimizer
optimizer_weight_decay: 0.001  # Weight decay (L2 regularization) helps prevent overfitting

use_scheduler: True # True, False
scheduler_type: 'reducelronplateau'  # Type of learning rate scheduler, can be 'reducelronplateau', 'steplr', or 'cosineannealing'
scheduler_step_size: 30  # Step size for StepLR, defines the number of epochs before the next LR decay
scheduler_gamma: 0.8  # Decay rate for learning rate under StepLR
initializer: 'he'  # He initialization method (Kaiming initialization)

contrast: 1

# # scheme_i constants
# C1: 3.0
# C2: 3.0
# beta: 1
# cnst: 1 


